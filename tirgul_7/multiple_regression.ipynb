{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "\n",
    "1. Linear regression with multiple predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression\n",
    "Multiple or multivariate linear regression is a case of linear regression with two or more independent variables.\n",
    "\n",
    "If there are just two independent variables, the estimated regression function is 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂. It represents a regression plane in a three-dimensional space. The goal of regression is to determine the values of the weights 𝑏₀, 𝑏₁, and 𝑏₂ such that this plane is as close as possible to the actual responses and yield the minimal SSR.\n",
    "\n",
    "The case of more than two independent variables is similar, but more general. The estimated regression function is 𝑓(𝑥₁, …, 𝑥ᵣ) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ +𝑏ᵣ𝑥ᵣ, and there are 𝑟 + 1 weights to be determined when the number of inputs is 𝑟.\n",
    "\n",
    "https://realpython.com/linear-regression-in-python/#multiple-linear-regression\n",
    "\n",
    "https://datatofish.com/multiple-linear-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the birthweight dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "file = \"./birthweight.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher\n",
    "\n",
    "Last time we examined the relationship between head circumfrence and birthweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df.Birthweight, y=df.Headcirc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df.Birthweight)\n",
    "Y = df.Headcirc\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's add another predicting variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the relationship between length and head circumfrence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df.Length, y=df.Headcirc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine our two 'predictor' variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group x variables\n",
    "X = df[['Birthweight', 'Length']]\n",
    "\n",
    "# y variable\n",
    "Y = df['Headcirc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using statsmodels\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, Y)\n",
    "\n",
    "print('Intercept: ', regr.intercept_)\n",
    "print('Coefficients: ', regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which package should we use?\n",
    "\n",
    "In general, scikit-learn is designed for machine-learning, while statsmodels is made for rigorous statistics.\n",
    "\n",
    "https://medium.com/@hsrinivasan2/linear-regression-in-scikit-learn-vs-statsmodels-568b60792991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonferroni correction for multiple hypotheses\n",
    "\n",
    "The Bonferroni correction is a multiple-comparison correction used when several dependent or independent statistical tests are being performed simultaneously (since while a given alpha value alpha may be appropriate for each individual comparison, it is not for the set of all comparisons). In order to avoid a lot of spurious positives, the alpha value needs to be lowered to account for the number of comparisons being performed.\n",
    "\n",
    "In multiple hypothesis testing there are two kinds of errors that must be to considered:\n",
    "1. Type I error: The rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\")\n",
    "2. Type II error (False negative): The non-rejection of a false null hypothesis (also known as a \"false negative\" finding or conclusion; example: \"a guilty person is not convicted\"\n",
    "\n",
    "https://mathworld.wolfram.com/BonferroniCorrection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from gene editing\n",
    "\n",
    "Background: A researcher analyzes thousands of genese to identify \"differentially expressed genes\" between two groups (e.g., normal vs. treated), which could alter biological mechanisms of a species in response to a particular treatment.\n",
    "\n",
    "If we analyze 10,000 results with a signifance level of 0.05, then we should expect hundreds of false positives.\n",
    "\n",
    "To control false discoveries from multiple hypothesis testing, it is imperative to adjust the significance level (α) to reduce the probability of getting Type I error.\n",
    "\n",
    "https://www.reneshbedre.com/blog/multiple-hypothesis-testing-corrections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# generate 500 random p-values\n",
    "rand_num = np.random.random(500)\n",
    "\n",
    "# alpha value\n",
    "alpha = 0.05\n",
    "\n",
    "# without correction, how many times would we reject the null hypothesis?\n",
    "print(len(rand_num[np.where(rand_num<alpha)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bonefrroni correction \n",
    "p_adjusted = multipletests(pvals=rand_num, alpha=alpha, method='bonferroni')\n",
    "print(len(p_adjusted[1][np.where(p_adjusted[1]<alpha)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4f6be21fc92d2a00f9ca51bcfc707caddf6cb807b3396ca594a2548e90cdcf4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
